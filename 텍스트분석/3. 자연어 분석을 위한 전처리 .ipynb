{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"1. 자연어 분석을 위한 전처리 .ipynb","private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyOAsIxCwtrsgXQ4Esqk2/MK"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# 자연어 데이터 처리\n","- nltk 모듈\n","- konlpy 모듈: 한국어 처리 모듈\n","\n","### 토큰화(Tokenization)\n","- 크롤링 등으로 얻은 데이터를 사용하고자 하는 용도에 맞게 처리하는 과정\n","- 토큰화(Tokenization)/정제(Cleaning)/정규화(Normalization) 진행\n","- 토큰화: 주어진 데이터에서 토큰(token)이라 불리는 단위로 나누는 작업을 토큰화(tokenization)라고 함\n","- 단어 토큰화의 경우 token은 단어를 의미한다고 생각하면 됨\n","\n","### 한글 형태소 분석\n","- 형태소(morpheme): 뜻을 가진 가장 작은 말의 단위\n","* 자립 형태소 : 접사, 어미, 조사와 상관없이 자립하여 사용할 수 있는 형태소(체언(명사, 대명사, 수사), 수식언(관형사, 부사), 감탄사 등)\n","* 의존 형태소 : 다른 형태소와 결합하여 사용되는 형태소. 접사, 어미, 조사, 어간 등을 말함"],"metadata":{"id":"uu3JaPNAvCrs"}},{"cell_type":"markdown","source":["#### Konlpy 형태소 분석"],"metadata":{"id":"6avQeaL0vsdU"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"NR6MNwI8u6-g"},"outputs":[],"source":["# 한글 폰트 설치 및 재실행\n","!sudo apt-get install -y fonts-nanum\n","!sudo fc-cache -fv\n","!rm ~/.cache/matplotlib -rf"]},{"cell_type":"code","source":["# nltk, Konlpy 설치\n","#!pip install nltk\n","#!pip install Konlpy"],"metadata":{"id":"kxXHck6Kv-WU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## konlpy을 이용한 형태소 확인\n","from konlpy.tag import Okt\n","\n","text=\"한국어 분석을 시작합니다. 형태소별로 추출합니다.\"\n","\n","okt=Okt()\n","\n","# 명사 추출\n","okt.nouns(text)  "],"metadata":{"id":"ts9i3KVdv_TO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Okt 형태소 분석기 토큰화 기능\n","- morphs : 형태소 추출\n","-  pos : 품사 태깅(Part-of-speech tagging)\n","-  nouns : 명사 추출"],"metadata":{"id":"CY3RQ6lUOyWB"}},{"cell_type":"code","source":["# 단어를 분류하고 형태소별로 출력\n","okt.pos(text)"],"metadata":{"id":"VHE3g6kmv_fQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 파일을 가져와 처리하기\n","- 파일: it-life-hack-6292880.txt"],"metadata":{"id":"rGeqyJURyJ6j"}},{"cell_type":"code","source":["# 파일 오픈\n","f = open('./data/it-life-hack-6292880.txt', encoding='utf-8')\n","\n","# 파일 내용 읽어오기\n","text = f.read()\n","\n","# 데이터 내용 확인하기\n","print(text)\n","\n","# 파일 닫기\n","f.close()"],"metadata":{"id":"7x7Cu0bxx5W6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from konlpy.tag import Okt\n","\n","# 인스턴스 객체 생성\n","okt = Okt()\n","\n","# 형태소(토큰) 분석\n","tokens = okt.pos(text)\n","#print(tokens)\n","\n","for token in tokens:\n","    print(token)"],"metadata":{"id":"NqIaHBGuz6_S"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 텍스트 데이터 정제 및 정규화\n","- 정제(cleaning) : 갖고 있는 코퍼스로부터 노이즈 데이터를 제거\n","- 정규화(normalization) : 표현 방법이 다른 단어들을 통합시켜서 같은 단어로 만듬\n","\n","1. 규칙에 기반한 표기가 다른 단어들의 통합\n","2. 영문인 경우 대, 소문자 통합\n","3. 불필요한 단어(노이즈 데이터)의 제거\n"," * 등장 빈도가 적은 단어\n"," * 길이가 짧은 단어\n","\n","- 정규 표현법을 통한 노이즈 데이터 제거 "],"metadata":{"id":"kY2dECyG1I0A"}},{"cell_type":"code","source":["import re\n","\n","# 숫자, 영문자 등 문자열 제거\n","reg_text = re.sub(r'[0-9a-zA-Z]+', \"\", text)\n","print(reg_text)\n","print(\"===============================================\")\n","\n","# 불필요한 기호 제거\n","reg_text = re.sub(r'[:;+/\\.,-![]]', \"\", reg_text)\n","print(reg_text)"],"metadata":{"id":"zytYtspt0xr_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### \\n/\\s를  \" \" 로 변경\n","reg_text = re.sub(r'[\\n\\s]+', \"\", reg_text)\n","print(reg_text)"],"metadata":{"id":"9fG8oMLO2TBJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 한국어 어간 어미 추출\n","- 어간(stem) : 용언(동사, 형용사)을 활용할 때, 원칙적으로 모양이 변하지 않는 부분\n","- 어미(ending): 용언의 어간 뒤에 붙어서 활용하면서 변하는 부분"],"metadata":{"id":"QZhpV2SNQVEr"}},{"cell_type":"code","source":["from konlpy.tag import Okt\n","\n","# 인스턴스 객체 생성\n","okt = Okt()\n","\n","# 숫자, 영문자 등 문자열 제거\n","reg_text = re.sub(r'[^가-힣]', \"\", text)\n","print(reg_text)\n","\n","# 형태소(토큰) 분석\n","tokens = okt.morphs(reg_text)\n","for token in tokens:\n","    print(token)"],"metadata":{"id":"Fav459LL4Scu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 불용어(Stopword) 제거\n","- 갖고 있는 데이터에서 유의미한 단어 토큰만을 선별하기 위해서는 큰 의미가 없는 단어 토큰을 제거하는 작업\n","- 한글 보편적 불용어: https://www.ranks.nl/stopwords/korean\n"],"metadata":{"id":"NnLIzEWjQyjU"}},{"cell_type":"code","source":["f=open('Stopwords.txt', encoding='utf-8')\n","stopword=f.read()\n","print(stopword.split(\"\\n\"))\n","stopword=stopword.split(\"\\n\")"],"metadata":{"id":"WjLwq6_TX77g"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokens_word =[]\n","\n","for token in tokens:\n","    if token not in stopword:\n","        tokens_word.append(token)\n","\n","tokens_word"],"metadata":{"id":"qwQUkJdlX8Co"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"WUcoG_jKX8IC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 단어에 대한 빈도수"],"metadata":{"id":"KGYEEo1r8BAK"}},{"cell_type":"code","source":["from collections import Counter\n","from konlpy.tag import Okt\n","\n","okt = Okt()\n","\n","tokens_pos = okt.pos(reg_text)\n","tokens_nouns = okt.nouns(reg_text)\n","\n","tokens_nouns\n","\n","# 단어에 대한 출현 건수 확인(빈도수)\n","cnt = Counter(tokens_nouns)\n","cnt   # 데이터 빈도스를 딕셔너리 구조로 출력"],"metadata":{"id":"qpqjaUkL7wqz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 상위 50개\n","print(cnt.most_common(50))\n","#print(cnt.most_common())"],"metadata":{"id":"26Me3c0L9yRv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","\n","df = pd.DataFrame(cnt.most_common())\n","df"],"metadata":{"id":"NoWl9wtP_Wwr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 워드클라우드"],"metadata":{"id":"ZtgGAEIccdA_"}},{"cell_type":"code","source":[""],"metadata":{"id":"ns4Q8pNZRYOb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"BzxtF67ccxqi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 기사 전체를 이용한 형태소 분석"],"metadata":{"id":"mTrq3-XYBPtO"}},{"cell_type":"code","source":["import os\n","import re\n","from konlpy.tag import Okt\n","import pandas as pd"],"metadata":{"id":"QZtZ9ovg_Wz_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["f=open('Stopwords.txt', encoding='utf-8')\n","stopword=f.read()\n","print(stopword.split(\"\\n\"))\n","stopword=stopword.split(\"\\n\")"],"metadata":{"id":"rx1rVpUmltAv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["okt = Okt()\n","\n","# 기사별 폴더 지정\n","fdir = ['it-life', 'move']\n","\n","doc_tmp = []   # 데이터 설명변수\n","label = []     # 레이블(목적) 변수\n","\n","# 각 폴더에 파일을 각각 읽어와 표시\n","for i, v in enumerate(fdir):\n","\n","    # 폴더에 있는 파일 목록 가져오기\n","    files = os.listdir(\"./\" + v )\n","    \n","    # 파일을 하나씩 읽어오기\n","    for file in files:\n","        tmp1 = []      # 파일단위 token 저장\n","        tmp2 = \"\"      # 파일단위 token 텍스트 저장\n","\n","        f = open(\"./\" + v + \"/\" + file, 'r', encoding='utf-8')\n","        text = f.read()\n","\n","        # 불필요한 문자열 제거하기(정규 표현식 사용)\n","        reg_text = re.sub(r'[0-9a-zA-Z]', \"\", text)    # [0-9a-zA-Z] => \\w\n","        reg_text = re.sub(r'[ \\t\\n\\r]', \"\", reg_text)  # [ \\t\\n\\r\\f\\v]  => [\\s]\n","        reg_text = re.sub(r'[::/\\-+.[]!?■]', \"\", reg_text)\n","\n","        # 제거된 문자열을 기준으로 명사만 추출하는 형태소 분석\n","        tokens=okt.nouns(reg_text)\n","        for token in tokens:\n","            if token not in stopword:   # 불용어 처리\n","                tmp1.append(token)      # tmp1 리스트에 추가\n","            \n","        tmp2 = \" \".join(tmp1)           # 리스트 형식을 \" \"로 구분된 텍스트로 변경\n","        doc_tmp.append(tmp2)            # 기사별/파일별 로 단어 추가\n","\n","        label.append(i)   # 레이블에 폴더 index 값 추가\n","        f.close()         # 파일 닫기\n","\n","pd.DataFrame(doc_tmp).head()  # DataFrame 으로 변경후 확인"],"metadata":{"id":"UKD7d88t_W4W"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 첫번째 기사/마지막 기사 내용 확인하기\n","print(doc_tmp[0])\n","print(doc_tmp[-1])"],"metadata":{"id":"nphqdpma-aTE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 첫번째 기사/마지막 기사에 대한 레이블 확인\n","print(label[0])\n","print(label[-1])"],"metadata":{"id":"TkpMUpRx-sAk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 전체 데이터 개수 확인\n","print(len(doc_tmp))"],"metadata":{"id":"b-OfnwEDs1ie"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(label)"],"metadata":{"id":"B4rFkN9FvCoF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pd.concat([pd.DataFrame(doc_tmp), pd.DataFrame(label)], axis=1).head()"],"metadata":{"id":"cGesbStyu_z4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","\n","news_data = np.concatenate((np.array(doc_tmp).reshape(400, 1), np.array(label).reshape(-1, 1)), axis=1)\n","news_data[:5]"],"metadata":{"id":"hjoF-7lvs1ld"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["np.array(doc_tmp).reshape(-1,1).shape"],"metadata":{"id":"2GRE751Ns1oC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 문서단어행렬(Document-Term Matrix, DTM)"],"metadata":{"id":"29AMOp3c565U"}},{"cell_type":"code","source":["# 첫번째 데이터에 대한 빈도수\n","from collections import Counter\n","\n","word_c1 = Counter(doc_tmp[0].split())\n","print(word_c1)\n","word_c1_lst=word_c1.most_common()\n","print(word_c1_lst)\n","\n","word_c1_df = pd.DataFrame(word_c1_lst)\n","display(word_c1_df.head())\n","\n","word_c1_df=word_c1_df.set_index(0)\n","display(word_c1_df.head())\n","\n","word_c1_df1=word_c1_df.T      # word_c1_df.transpose()\n","word_c1_df1"],"metadata":{"id":"8VXh3xqks1qh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 2번째 데이터가져와 행/열 변환 후 기존 데이터에 추가하기\n","from collections import Counter\n","\n","word_c1_df1 = pd.DataFrame()\n","\n","for i in range(len(doc_tmp)):\n","    word_c1 = Counter(doc_tmp[i].split())\n","    word_c1_lst=word_c1.most_common()\n","    word_c1_df = pd.DataFrame(word_c1_lst)\n","    word_c1_df=word_c1_df.set_index(0)\n","    # 기존 1번째 데이터의 행/열 변환 값에 2번째 데이터 추가\n","    word_c1_df1= pd.concat([word_c1_df1, word_c1_df.T], axis=0, ignore_index=True)\n","\n","display(word_c1_df1)\n"],"metadata":{"id":"9saDUHpzs1ty"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Na값 0으로 변경\n","word_c1_df1=word_c1_df1.fillna(0)\n","\n","# 모든 데이터를 int형으로 변경\n","word_c1_df1=word_c1_df1.astype(int)\n","display(word_c1_df1.head())\n","\n","\n","# 단어 길이가 2개 이상인 데이터만 추출\n","colName = word_c1_df1.columns\n","new_colName = []\n","for cn in colName:\n","    if len(cn) >= 2:\n","        new_colName.append(cn)\n","\n","# 열 이름을 기준으로 오름차순 정렬\n","\n","new_colName=new_colName.sort()\n","\n","# 필요한 데이터만 추출해 word_c1_df_new에 저장\n","word_c1_df_new = word_c1_df1[new_colName].copy()\n","print(len(word_c1_df_new.columns))"],"metadata":{"id":"iNo1V2zv0Kx6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 컬럼명을 index로 변경\n","colName_num =[]\n","\n","for i, name in enumerate(new_colName):\n","    colName_num.append(i)\n","\n","word_c1_df_new.columns = colName_num"],"metadata":{"id":"zqlof5ylSxwV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 어휘 빈도 확인(TF)\n","display(word_c1_df_new.head())\n","word_c1_df_new.values"],"metadata":{"id":"lJt8Y17X4y0A"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 문서 역빈도(IDF)"],"metadata":{"id":"LXDg7P1cQLVK"}},{"cell_type":"code","source":["# 역문서 빈도(IDF)\n","# 특정 단어가 가지는 문서에서 출현하는 빈도수: 기사 1개에서 단어가 있으면 1, 없음면 0 => 모든 기사를 기준으로 더한 값\n","# 총문서수(행의 갯수)\n","# log(총문서수/(특정 단어가 가지는 문서에서 출현하는 빈도수)) + 1\n","\n"],"metadata":{"id":"90UsxqdcKBaA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"JPZYCgzvKB2c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","from sklearn.feature_extraction.text import CountVectorizer\n","\n","# 단어의 출현건수 카운트\n","cv = CountVectorizer()\n","doct_cv = cv.fit_transform(np.array(doc_tmp))\n","doct_cnt = doct_cv.toarray()\n","\n","print(doct_cnt[:5])\n","\n","display(pd.DataFrame(doct_cnt).head())"],"metadata":{"id":"emGlrfGA5tUu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 데이터 사이즈\n","print(pd.DataFrame(doct_cnt).shape)"],"metadata":{"id":"4ehSV5WR8cj-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(cv.get_feature_names()[:50])\n","print(colName[:50])"],"metadata":{"id":"urzBCUT79uAs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 단어의 출현 빈도수 카우트\n","# 단어의 출현건수 카운트\n","cv = CountVectorizer(min_df=0.01, max_df=0.5)\n","doct_cv = cv.fit_transform(np.array(doc_tmp))\n","doct_cnt = doct_cv.toarray()\n","\n","display(pd.DataFrame(doct_cnt).head())"],"metadata":{"id":"pvm7C00D_1yy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pd.DataFrame(doct_cnt).shape"],"metadata":{"id":"EgPCLL6RHWAL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(cv.get_feature_names()[:50])"],"metadata":{"id":"G1lGa7xCIGaH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 어휘빈도-문서역빈도(TF-IDF) 분석(교재 173p~)\n","- DF-IDF = TF * IDF\n","- 엑셀파일 참조"],"metadata":{"id":"PQUb4HnAKFxc"}},{"cell_type":"code","source":["# TF-IDF: TF * IDF\n","# 판다스 또는 numpy를 이용해 직접 작업하세요.\n","\n"],"metadata":{"id":"EaQgvcUfT3b1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","## TF-IDF\n","tv = TfidfVectorizer(min_df=0.01, max_df=0.5, sublinear_tf=True)\n","doc_tv = tv.fit_transform(np.array(doc_tmp))\n","doc_Tfidf = doc_tv.toarray()\n","\n","display(pd.DataFrame(doc_Tfidf).head())"],"metadata":{"id":"sPNYRjqlILfM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"Prb_9_4GRzRQ"},"execution_count":null,"outputs":[]}]}